{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('miniconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MAVEN Expansion\n",
    "This jupyter notebook script adds lemma, PoS, dependency parsing and some other information onto MAVEN dataset with [Stanza](https://stanfordnlp.github.io/stanza)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './maven_data'\n",
    "files = ('valid.jsonl', 'test.jsonl', 'train.jsonl')\n",
    "output_dir = './maven_data'\n",
    "suffix = '_expanded'"
   ]
  },
  {
   "source": [
    "## Resources"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Uncomment the following line if stanza English model is not downloaded.\n",
    "# stanza.download('en')"
   ]
  },
  {
   "source": [
    "## Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse', tokenize_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc_item):\n",
    "    content_item = doc_item['content']\n",
    "    for sentence_item in content_item:\n",
    "        sentence_tokens = sentence_item['tokens']\n",
    "        sentence_result = nlp([sentence_tokens]).sentences[0]\n",
    "        sentence_result = sentence_result.to_dict()\n",
    "        for token_result in sentence_result:\n",
    "            token_result['id'] -= 1\n",
    "            token_result['head'] -= 1\n",
    "            token_result.pop('text')\n",
    "            token_result.pop('misc')\n",
    "        sentence_item['token_info'] = sentence_result\n",
    "    return doc_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_name):\n",
    "    output_file_name = file_name[:-6] + suffix + '.jsonl'\n",
    "\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as of:\n",
    "            for line in f:\n",
    "                if line.strip() == '':\n",
    "                    continue\n",
    "                doc_item = json.loads(line)\n",
    "                expanded_doc_item = process_doc(doc_item)\n",
    "\n",
    "                expanded_line = json.dumps(expanded_doc_item, ensure_ascii=False)\n",
    "                of.write(expanded_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print('processing %s' % file)\n",
    "    process_file(file)\n",
    "    print('%s done' % file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}